---
title: "Second Method"
output: html_document
---

```{r, echo=TRUE}

#### Data preparation ####
library(dplyr)
library(tidyr)
library(rpart)
library(randomForest)
library(ggplot2)
library(gbm)

df <- read.csv("~/Desktop/train.csv", stringsAsFactors = F)
row.names(df) <- df$Id
df <- df[,-1]
df[is.na(df)] <- 0
for(i in colnames(df[,sapply(df, is.character)])){
    df[,i] <- as.factor(df[,i])
}

test.n <- sample(1:nrow(df), nrow(df)/3, replace = F)
test <- df[test.n,]
train <- df[-test.n,]
rm(test.n, df)
RMSE <- function(x,y){
    a <- sqrt(sum((log(x)-log(y))^2)/length(y))
    return(a)
}


#### Regression Trees, "anova" ####
model1 <- rpart(SalePrice ~., data = train, method = "anova")
predict1 <- predict(model1, test)
RMSE1 <- RMSE(predict1, test$SalePrice)
RMSE1 <- round(RMSE1, digits = 3)
plot1 <- predict1-test$SalePrice
RMSE1


#### Random Forests, "anova" ####
model2 <- randomForest(SalePrice ~., data = train, method = "anova",
                      ntree = 300,
                      mtry = 26,
                      replace = F,
                      nodesize = 1,
                      importance = T)
predict2 <- predict(model2, test)
RMSE2 <- RMSE(predict2, test$SalePrice)
RMSE2 <- round(RMSE2, digits = 3)
plot2 <- predict2-test$SalePrice
RMSE2

#### Plot: "Difference between predict and real values" ####

data_plot <- data.frame("regression trees" = plot1,
                        "random forests" = plot2)
data_plot$Id <- row.names(data_plot)
data_plot <- gather(data_plot, method, value, - Id)
data_plot$method <- as.factor(data_plot$method)
levels(data_plot$method) <- c(paste0("Random Forests (", RMSE2, ")"),
                              paste0("Regression Trees (", RMSE1, ")"))

ggplot(data_plot, aes(x = Id, y = value, colour = method))+
    geom_point(alpha = 0.7, size = 2)+
    ggtitle("The difference between predict and real prices")+
    labs(x = "Buyer Id", y = "The difference between prices", colour = " ")+
    scale_x_discrete(breaks = c(0))+
    theme(legend.position = "top",
          legend.text = element_text(size = 12),
          axis.text.x = element_blank(), 
          axis.title.x = element_text(size = 14),
          axis.text.y = element_text(size = 14), 
          axis.title.y = element_text(size = 14),
          title = element_text(size = 16))

```


```{r, echo=TRUE}
library(ggplot2) 
library(readr) 
library(Hmisc)
library("caret")
library("rpart")
library("tree")
library("randomForest")
library("e1071")
data= read.csv("~/Desktop/train.csv", header = T, na.strings= "NA")
data2 = read.csv("~/Desktop/test.csv", header = T, na.strings= "NA")

# Calculate Missing Data
MissingPercentage <- function(x){ sum(is.na(x)/length(x)*100 )}  

sort( apply(data, 2, MissingPercentage ), decreasing = TRUE )

# Check for #NA and Delete
sort( sapply(data, function(x) sum(is.na(x))) , decreasing = TRUE )
library(dplyr)
data = select(data, -c(PoolQC, MiscFeature, Alley, Fence, FireplaceQu, LotFrontage))

# CART: Classification, test original and Imputed data
library(mice)
imp_data <- mice(data, m=1, method='cart', printFlag= FALSE )
table( data$GrageType)
table( imp_data$imp$GrageType)

# Merge to original data
data_complete <- complete(imp_data)

# Confirm no NAs
sum( sapply( data_complete, function(x) { sum(is.na(x))} ) )
#0, success
# save data to CSV
write.csv(data_complete , file = "data_complete.csv" )
# data_complete = read.csv("data_complete.csv", header = T )

set.seed(1)
train = sample( 1:nrow(data_complete), nrow(data_complete)/2 )
test = -train

traindata = data_complete[train, ]
testdata = data_complete[test, ]
RMSE <- function(x,y){
    a <- sqrt(sum((log(x)-log(y))^2)/length(y))
    return(a)
}

####Stepwise####
library(leaps)
regfit.full=regsubsets(SalePrice~
                         LotArea+OverallQual+OverallCond+YearBuilt+BsmtQual+BsmtFinSF1+
                         BsmtFinSF2+BsmtUnfSF+X1stFlrSF+X2ndFlrSF+BedroomAbvGr+
                         KitchenAbvGr+KitchenQual+GarageCars+PoolArea,data=traindata,nvmax=19)

summary_reg = summary(regfit.full)
names(summary_reg)
summary_reg$rsq # r square
summary_reg$adjr2
par(mfrow =c(2,2))
plot(summary_reg$rss ,xlab=" Number of Variables ",ylab=" RSS",
     type="l") # type l connect dot with line 
plot(summary_reg$adjr2 ,xlab =" Number of Variables ",
     ylab=" Adjusted R-Square",type="l")
which.max(summary_reg$adjr2)

which.min(summary_reg$rss)
which.min(summary_reg$bic)

regfit.bwd=regsubsets (SalePrice~.,nvmax =50,really.big = TRUE,
                       method="backward",data=traindata)
summary_reg2= summary(regfit.bwd)
which.min(summary_reg2$rss)

par(mfrow=c(2,2))
plot(summary_reg2$adjr2,xlab="Number of variables", ylab="Adjusted R-Square", type="l")
points(1:50,summary_reg2$rsq[1:50], col="red",cex=2,pch=20)
plot(summary_reg2$rss,xlab="Number of variables", ylab="RSS", type="l")
points(1:50,summary_reg2$rss[1:50], col="blue",cex=2,pch=20)

coef(regfit.bwd,20)

# since no predict function for regsubsets
predict.regsubsets =function (object ,newdata ,id ,...){
  form=as.formula (object$call[[2]])
  mat=model.matrix (form,newdata)
  coefi =coef(object,id=id)
  xvars =names (coefi)
  mat[,xvars ]%*% coefi
}

best_subset_pred = predict.regsubsets (regfit.bwd,testdata,20)

testRMSE <- RMSE(best_subset_pred, testdata$SalePrice)
testRMSE #0.2688095

####Ridge####
# convert any qualitative variables to dummy variables
x=model.matrix(SalePrice~.,data_complete)[,-1] # get rid of intercept column
y=data_complete$SalePrice

# split the dataset into training and testing
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = -train

training_x = x[train,]
testing_x = x[test,]

training_y = y[train]
testing_y = y[test]

# glmnet package to perform ridge and lasso
library(glmnet)
#lambda 10^10 to 10^-2
grid = 10^seq (10,-2,length =100)
ridge_model = glmnet(training_x,training_y,alpha = 0,lambda = grid, standardize = FALSE)
dim(coef(ridge_model)) # 233 for predictor, 100 for lambda

plot(ridge_model, xvar = "lambda", label = TRUE)

#Best Lambda
set.seed(2)
cv_error = cv.glmnet(training_x, 
                     training_y, 
                     alpha = 0) #default 10 fold cv
plot(cv_error) # cross-validation curve (red dotted line), upper and lower standard deviation curves 

best_lambda = cv_error$lambda.min
best_lambda

# model with best lambda
model_coef = predict(ridge_model, 
                     type = "coefficients",
                     s= best_lambda)

### test the model 
predicted_y = predict(ridge_model, 
                      s= best_lambda,
                      newx = testing_x)
### RMSE
RMSE_RIDGE=RMSE(predicted_y, testing_y)
RMSE_RIDGE #0.1755976

####LASSO####
# convert any qualitative variables to dummy variables
x=model.matrix(SalePrice~.,data_complete)[,-1] # get rid of intercept column
y=data_complete$SalePrice

# split the dataset into training and testing
set.seed(1)
train = sample(1:nrow(x), nrow(x)/2)
test = -train

training_x = x[train,]
testing_x = x[test,]

training_y = y[train]
testing_y = y[test]

library(glmnet)
grid = 10^seq (10,-2,length =100)

lasso_model = glmnet(training_x, 
                     training_y, 
                     alpha =1,
                     lambda=grid,
                     standardize=FALSE)

plot(lasso_model, xvar = "lambda",label = TRUE)

set.seed(2)
cv_error = cv.glmnet(training_x, 
                     training_y, 
                     alpha = 1)
best_lambda = cv_error$lambda.min
best_lambda 

plot(cv_error)

### OUR FINAL LASSO 
model_coef = predict(lasso_model, 
                     type = "coefficients",
                     s= best_lambda)

### test the model 
predicted_y = predict(lasso_model, 
                      s= best_lambda,
                      newx = testing_x)
### RMSE
RMSE_LASSO<- RMSE(predicted_y, testing_y)
RMSE_LASSO #0.1782259

#### Binary Logistic Regression ####

# create a binary variable "sale_above_avg", delete SalePrice
data_complete$sale_above_avg =ifelse(data_complete$SalePrice>=mean(data_complete$SalePrice),1,0)

library(dplyr)
data_complete_l=select(data_complete,-SalePrice)

set.seed(1) # for reproducibility
train = sample(1:nrow(data_complete),nrow(data_complete)/2)
#train = sample(nrow(data_complete), 1000)
test = -train
traindata = data_complete_l[train,]
testdata = data_complete_l[test,]

# fit the model
glm.fit=glm(sale_above_avg~
              LotArea+OverallQual+OverallCond+YearBuilt+BsmtQual+
              X1stFlrSF+X2ndFlrSF+BedroomAbvGr+
              KitchenAbvGr+KitchenQual+GarageCars,
            data=traindata,family =binomial)
summary(glm.fit) 
anova(glm.fit, test="Chisq")

# Prediction and accuracy test
fitted.results <- predict(glm.fit,newdata=testdata,type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)

misClasificError <- mean(fitted.results != testdata$sale_above_avg)
print(paste('Accuracy',1-misClasificError))


table=table(fitted.results,testdata$sale_above_avg)
table
error_rate <- (table[1,2]+table[2,1])/sum(table)
error_rate #0.0739726

#####Regression Trees RMSE= 0.233; Random Forest RMSE= 0.15; Stepwise RMSE=0.2688095; Ridge RMSE=0.1755976; Lasso RMSE=0.1782259; Binary Logistic Regression has Accuracy 0.926, error rate 0.0739726.#####
```
